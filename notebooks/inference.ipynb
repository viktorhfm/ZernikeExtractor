{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with experimental and synthetic patterns\n",
    "\n",
    "by Victor Hugo Flores Mu√±oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from sklearn.utils import shuffle\n",
    "from zernike import RZern\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza(A):\n",
    "    mask = np.isnan(A)\n",
    "    B = np.nan_to_num(A, nan=0)\n",
    "    C = (B - B.min())/(B.max() - B.min()) * 2 - 1\n",
    "    C[mask] = 0.0\n",
    "    return C\n",
    "\n",
    "def normaliza_pos(A):\n",
    "    mask = np.isnan(A)\n",
    "    B = np.nan_to_num(A, nan=0)\n",
    "    C = (B - B.min())/(B.max() - B.min())\n",
    "    C[mask] = 0.0\n",
    "    return C\n",
    "\n",
    "def generate_mask(reference):\n",
    "    mask = np.isnan(reference)\n",
    "    mask = np.logical_not(mask)\n",
    "    return mask\n",
    "\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "cart = RZern(2)\n",
    "ddx = np.linspace(-1.0, 1.0, WIDTH)\n",
    "ddy = np.linspace(-1.0, 1.0, HEIGHT)\n",
    "xv, yv = np.meshgrid(ddx, ddy)\n",
    "cart.make_cart_grid(xv, yv)\n",
    "num_coef = cart.nk\n",
    "print(\"Numero de coeficientes: \", num_coef)\n",
    "Phi = cart.eval_grid(np.array([0,1,0,0,0,0]), matrix=True)\n",
    "MASK = generate_mask(Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneradorZ(num_samples=50000, num_coef=10, h=256, w=256):\n",
    "    y = np.empty((num_samples, num_coef))\n",
    "    X = np.empty((num_samples, h, w))\n",
    "    Xcos = np.empty((num_samples, h, w))\n",
    "    for k in range(num_samples):\n",
    "        y[k] = [\n",
    "            random.random() * 20 - 10,\n",
    "            random.random() * 30 - 15,\n",
    "            random.random() * 30 - 15,\n",
    "            random.random() * 30 - 15,\n",
    "            random.random() * 30 - 15,\n",
    "            random.random() * 30 - 15\n",
    "        ]\n",
    "        Phi = cart.eval_grid(y[k], matrix=True)\n",
    "        X[k,:,:] = normaliza_pos(Phi)\n",
    "        Xcos[k,:,:] = normaliza_pos(np.cos(Phi))\n",
    "    return X, y, Xcos\n",
    "\n",
    "def GeneradorZ_sparse(num_samples=50000, num_coef=10, h=256, w=256):\n",
    "    y = np.empty((num_samples,num_coef))\n",
    "    X = np.empty((num_samples, h, w))\n",
    "    Xcos = np.empty((num_samples, h, w))\n",
    "    for k in range(num_samples):\n",
    "        y[k] = np.zeros(num_coef)\n",
    "        n_terms = np.random.randint(1,num_coef)\n",
    "        for cont in range(n_terms):\n",
    "            index = np.random.randint(num_coef)\n",
    "            if index == 0:\n",
    "                y[k, index] = random.random() * 20 - 10\n",
    "            else:\n",
    "                y[k, index] = random.random() * 30 - 15\n",
    "        Phi = cart.eval_grid(y[k], matrix=True)\n",
    "        X[k,:,:] = normaliza_pos(Phi)\n",
    "        Xcos[k,:,:] = normaliza_pos(np.cos(Phi))\n",
    "    return X, y, Xcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "\n",
    "X1, y1, Xcos1 = GeneradorZ(num_samples=NUM_SAMPLES//2, \n",
    "                           num_coef=num_coef, \n",
    "                           h=HEIGHT, \n",
    "                           w=WIDTH)\n",
    "X2, y2, Xcos2 = GeneradorZ_sparse(num_samples=NUM_SAMPLES//2, \n",
    "                                  num_coef=num_coef, \n",
    "                                  h=HEIGHT, \n",
    "                                  w=WIDTH)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, y2))\n",
    "Xcos = np.concatenate((Xcos1, Xcos2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "X, y, Xcos= shuffle(X, y, Xcos)\n",
    "synthetic_dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "synthetic_dataset = synthetic_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Experimental Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_path = '../Datasets/Zernikes/Train/'\n",
    "fnames = os.listdir(experimental_path)\n",
    "clean_fnames = [fname for fname in fnames if '_c' not in fname]\n",
    "clean_fnames.sort()\n",
    "len(clean_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dict(path, destination_dict):\n",
    "    loaded_dict = json.load(open(path, 'r'))\n",
    "    for key in loaded_dict.keys():\n",
    "        destination_dict[key] = loaded_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_combinacion = '../Datasets/Zernikes/Zernikes_combinacion/Zernikes_coef_combinacion.json'\n",
    "path_puros = '../Datasets/Zernikes/Zernikes_puros/Zernikes_coef_puros.json'\n",
    "path_random = '../Datasets/Zernikes/Zernikes_random/Zernikes_coef_random.json'\n",
    "mega_json = {}\n",
    "json_to_dict(path_combinacion, mega_json)\n",
    "json_to_dict(path_puros, mega_json)\n",
    "json_to_dict(path_random, mega_json)\n",
    "print(len(mega_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for fname in clean_fnames:\n",
    "    X.append(cv2.imread(experimental_path + fname, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "    y.append(mega_json[fname])\n",
    "print(\"Loaded images: \", len(X))\n",
    "print(\"Loaded labels: \", len(y))\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X shape: \", X.shape) \n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_columns(arr, frm, to):\n",
    "    arr[:,[frm, to]] = arr[:,[to, frm]]\n",
    "\n",
    "swap_columns(y, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "X, y = shuffle(X, y)\n",
    "experimental_dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "experimental_dataset = experimental_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "loss_object_zernike = tf.keras.losses.MeanAbsoluteError()\n",
    "loss_object_phase = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "autoencoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "zernike_decoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "\n",
    "BETA = 100\n",
    "ALPHA = 1\n",
    "LAMBDA = 10\n",
    "\n",
    "def ae_loss(y_true, y_pred):\n",
    "    return BETA * loss_object(y_true, y_pred)\n",
    "\n",
    "\n",
    "def generate_zernike(coef, normalize=True):\n",
    "    Phi = cart.eval_grid(coef, matrix=True)\n",
    "    if normalize:\n",
    "        Phi = normaliza_pos(Phi)\n",
    "    return Phi\n",
    "\n",
    "def zernike2phi(coef):\n",
    "    Phi = tf.map_fn(\n",
    "        lambda x: generate_zernike(x, normalize=False),\n",
    "        coef\n",
    "    )\n",
    "    B = np.nan_to_num(Phi, nan=0)\n",
    "    return B\n",
    "\n",
    "def zernike2cos(coef):\n",
    "    Phi = tf.map_fn(\n",
    "        lambda x: generate_zernike(x, normalize=False),\n",
    "        coef\n",
    "    )\n",
    "    Phi = tf.math.cos(Phi)\n",
    "    Phi = tf.map_fn(lambda x: normaliza_pos(x), Phi)\n",
    "    return Phi\n",
    "\n",
    "def phase_loss(y_true, y_pred):\n",
    "    phi = zernike2phi(y_true)\n",
    "    hat_phi = zernike2phi(y_pred)\n",
    "    return ALPHA * loss_object_phase(phi, hat_phi)\n",
    "\n",
    "def cos_loss(y_true, y_pred):\n",
    "    phi = zernike2cos(y_true)\n",
    "    hat_phi = zernike2cos(y_pred)\n",
    "    return LAMBDA * loss_object_phase(phi, hat_phi)\n",
    "\n",
    "def zernike_loss(y_true, y_pred):\n",
    "    return ALPHA * loss_object_zernike(y_true, y_pred)\n",
    "\n",
    "def zernike2gradient(coef):\n",
    "    Phi = tf.map_fn(\n",
    "        lambda x: generate_zernike(x, normalize=False),\n",
    "        coef\n",
    "    )\n",
    "    Phi = tf.convert_to_tensor(\n",
    "        np.expand_dims(\n",
    "            np.nan_to_num(Phi), \n",
    "            axis=3\n",
    "        )\n",
    "    )\n",
    "    dx, dy = tf.image.image_gradients(Phi)\n",
    "    return dx, dy\n",
    "\n",
    "def grad_loss(y_true, y_pred):\n",
    "    dx_true, dy_true = zernike2gradient(y_true)\n",
    "    dx_pred, dy_pred = zernike2gradient(y_pred)\n",
    "    return LAMBDA * (0.5*loss_object(dx_true, dx_pred) + 0.5*loss_object(dy_true, dy_pred))\n",
    "\n",
    "def total_zernike_loss(y_true, y_pred):\n",
    "    return phase_loss(y_true, y_pred) + grad_loss(y_true, y_pred) + zernike_loss(y_true, y_pred)\n",
    "\n",
    "def model_loader(path, optimizer, loss):\n",
    "    model = tf.keras.models.load_model(path, compile=False)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model_loader('../models/ae_encoder.h5',\n",
    "                       autoencoder_optimizer,\n",
    "                       ae_loss)\n",
    "decoder = model_loader('../models/ae_decoder.h5',\n",
    "                       autoencoder_optimizer,\n",
    "                       ae_loss)\n",
    "zernike_decoder = model_loader('../models/zae_zernike_decoder.h5',\n",
    "                              zernike_decoder_optimizer,\n",
    "                              total_zernike_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zautoencoder = tf.keras.models.Model(\n",
    "    inputs=encoder.inputs, \n",
    "    outputs=zernike_decoder(encoder.outputs)\n",
    ")\n",
    "zautoencoder.compile(optimizer=zernike_decoder_optimizer, \n",
    "                     loss=total_zernike_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict with synthetic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_config.enable_numpy_behavior()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def error_estimator(y_true, y_pred):\n",
    "    return np.sqrt(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [1, 2, 3, 4, 5]\n",
    "example_input, example_target = next(iter(synthetic_dataset.take(100)))\n",
    "zernike_coefs = zautoencoder.predict(example_input)\n",
    "generated_zernike = zernike2phi(zernike_coefs)\n",
    "real_zernike = zernike2phi(example_target)\n",
    "plt.figure(figsize=(20, 16))\n",
    "for i in range(5):\n",
    "    y_true = normaliza_pos(real_zernike[samples[i]])*MASK\n",
    "    y_pred = normaliza_pos(generated_zernike[samples[i]])*MASK\n",
    "    error = error_estimator(y_true, y_pred)*MASK\n",
    "    experimental_pattern = example_input[samples[i]]\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(experimental_pattern, origin='lower')\n",
    "    # plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$I_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "    plt.subplot(4, 5, i + 6)\n",
    "    plt.imshow(y_true, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\phi_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(y_pred, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\hat\\phi_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.18)\n",
    "    plt.subplot(4, 5, i + 16)\n",
    "    plt.imshow(error, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\epsilon_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict With Experimental Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [1, 2, 3, 4, 5]\n",
    "example_input, example_target = next(iter(experimental_dataset.take(100)))\n",
    "zernike_coefs = zautoencoder.predict(example_input)\n",
    "generated_zernike = zernike2phi(zernike_coefs)\n",
    "real_zernike = zernike2phi(example_target)\n",
    "plt.figure(figsize=(20, 16))\n",
    "for i in range(5):\n",
    "    y_true = normaliza_pos(real_zernike[samples[i]])*MASK\n",
    "    y_pred = normaliza_pos(generated_zernike[samples[i]])*MASK\n",
    "    error = error_estimator(y_true, y_pred)*MASK\n",
    "    experimental_pattern = example_input[samples[i]]\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(experimental_pattern, origin='lower')\n",
    "    # plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$I_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "    plt.subplot(4, 5, i + 6)\n",
    "    plt.imshow(y_true, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\phi_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(y_pred, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\hat\\phi_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.18)\n",
    "    plt.subplot(4, 5, i + 16)\n",
    "    plt.imshow(error, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\epsilon_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
