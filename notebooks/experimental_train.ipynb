{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training With Experimental \n",
    "by Victor Hugo Flores Mu√±oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from zernike import RZern\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza(A):\n",
    "    mask = np.isnan(A)\n",
    "    B = np.nan_to_num(A, nan=0)\n",
    "    C = (B - B.min())/(B.max() - B.min()) * 2 - 1\n",
    "    C[mask] = 0.0\n",
    "    return C\n",
    "\n",
    "def normaliza_pos(A):\n",
    "    mask = np.isnan(A)\n",
    "    B = np.nan_to_num(A, nan=0)\n",
    "    C = (B - B.min())/(B.max() - B.min())\n",
    "    C[mask] = 0.0\n",
    "    return C\n",
    "\n",
    "def generate_mask(reference):\n",
    "    mask = np.isnan(reference)\n",
    "    mask = np.logical_not(mask)\n",
    "    return mask\n",
    "\n",
    "def json_to_dict(path, destination_dict):\n",
    "    loaded_dict = json.load(open(path, 'r'))\n",
    "    for key in loaded_dict.keys():\n",
    "        destination_dict[key] = loaded_dict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator uses this order:\n",
    "$$ [ Z_0^0, Z_1^1, Z_1^{-1}, Z_2^0, Z_2^{-2}, Z_2^2 ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "cart = RZern(2)\n",
    "ddx = np.linspace(-1.0, 1.0, WIDTH)\n",
    "ddy = np.linspace(-1.0, 1.0, HEIGHT)\n",
    "xv, yv = np.meshgrid(ddx, ddy)\n",
    "cart.make_cart_grid(xv, yv)\n",
    "num_coef = cart.nk\n",
    "print(\"Numero de coeficientes: \", num_coef)\n",
    "Phi = cart.eval_grid(np.array([0,1,0,0,0,0]), matrix=True)\n",
    "MASK = generate_mask(Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the Experimental Dataset\n",
    "This dataset uses the following order:\n",
    "$$ [ Z_0^0, Z_1^{-1}, Z_1^1, Z_2^0, Z_2^{-2}, Z_2^2 ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images without complements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_path = '../Datasets/Zernikes/Train/'\n",
    "fnames = os.listdir(experimental_path)\n",
    "clean_fnames = [fname for fname in fnames if '_c' not in fname]\n",
    "clean_fnames.sort()\n",
    "len(clean_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labels from json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_combinacion = '../Datasets/Zernikes/Zernikes_combinacion/Zernikes_coef_combinacion.json'\n",
    "path_puros = '../Datasets/Zernikes/Zernikes_puros/Zernikes_coef_puros.json'\n",
    "path_random = '../Datasets/Zernikes/Zernikes_random/Zernikes_coef_random.json'\n",
    "mega_json = {}\n",
    "json_to_dict(path_combinacion, mega_json)\n",
    "json_to_dict(path_puros, mega_json)\n",
    "json_to_dict(path_random, mega_json)\n",
    "print(len(mega_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for fname in clean_fnames:\n",
    "    X.append(cv2.imread(experimental_path + fname, cv2.IMREAD_GRAYSCALE) / 255.0)\n",
    "    y.append(mega_json[fname])\n",
    "print(\"Loaded images: \", len(X))\n",
    "print(\"Loaded labels: \", len(y))\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X shape: \", X.shape) \n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_columns(arr, frm, to):\n",
    "    arr[:,[frm, to]] = arr[:,[to, frm]]\n",
    "\n",
    "swap_columns(y, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the label asignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zernike_cos(coef, normalize=True):\n",
    "    Phi = cart.eval_grid(coef, matrix=True)\n",
    "    Phi = np.cos(Phi)\n",
    "    if normalize:\n",
    "        Phi = normaliza_pos(Phi)\n",
    "    return Phi\n",
    "\n",
    "def plot_real_and_ideal(index, normalize=True):\n",
    "    coef = y[index]\n",
    "    Phi = generate_zernike_cos(coef, normalize)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(normaliza_pos(X[index]))\n",
    "    # plt.title(f'{np.round(y[200], decimals=2)}')\n",
    "    plt.title(\"a) Experimental\", fontdict={'fontsize': 20}, y=-0.13)\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(Phi)\n",
    "    # plt.title(f'{np.round(y[200], decimals=2)}')\n",
    "    plt.title(\"b) Synthetic\", fontdict={'fontsize': 20}, y=-0.13)\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "sample = 200\n",
    "plot_real_and_ideal(sample, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "NUM_SAMPLES = len(X)\n",
    "\n",
    "num_samples_train = round(NUM_SAMPLES * .9)\n",
    "X, y = shuffle(X, y)\n",
    "X_train = np.expand_dims(X[:num_samples_train], axis=3)\n",
    "y_train = y[:num_samples_train]\n",
    "X_test = np.expand_dims(X[num_samples_train:], axis=3)\n",
    "y_test = y[num_samples_train:]\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "validation_split = 0.1\n",
    "num_samples = len(train_dataset)\n",
    "num_train = round(num_samples * (1 - validation_split))\n",
    "train_ds = train_dataset.take(num_train).batch(BATCH_SIZE)\n",
    "val_dataset = train_dataset.skip(num_train).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models\n",
    "Load the models trained with synthetic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "loss_object_zernike = tf.keras.losses.MeanAbsoluteError()\n",
    "loss_object_phase = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "autoencoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "zernike_decoder_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "\n",
    "BETA = 100\n",
    "ALPHA = 1\n",
    "LAMBDA = 10\n",
    "\n",
    "def ae_loss(y_true, y_pred):\n",
    "    return BETA * loss_object(y_true, y_pred)\n",
    "\n",
    "\n",
    "def generate_zernike(coef, normalize=True):\n",
    "    Phi = cart.eval_grid(coef, matrix=True)\n",
    "    if normalize:\n",
    "        Phi = normaliza_pos(Phi)\n",
    "    return Phi\n",
    "\n",
    "def zernike2phi(coef):\n",
    "    Phi = tf.map_fn(\n",
    "        lambda x: generate_zernike(x, normalize=False),\n",
    "        coef\n",
    "    )\n",
    "    B = np.nan_to_num(Phi, nan=0)\n",
    "    return B\n",
    "\n",
    "def zernike2cos(coef):\n",
    "    Phi = tf.map_fn(\n",
    "        lambda x: generate_zernike(x, normalize=False),\n",
    "        coef\n",
    "    )\n",
    "    Phi = tf.math.cos(Phi)\n",
    "    Phi = tf.map_fn(lambda x: normaliza_pos(x), Phi)\n",
    "    return Phi\n",
    "\n",
    "def phase_loss(y_true, y_pred):\n",
    "    phi = zernike2phi(y_true)\n",
    "    hat_phi = zernike2phi(y_pred)\n",
    "    return ALPHA * loss_object_phase(phi, hat_phi)\n",
    "\n",
    "def cos_loss(y_true, y_pred):\n",
    "    phi = zernike2cos(y_true)\n",
    "    hat_phi = zernike2cos(y_pred)\n",
    "    return LAMBDA * loss_object_phase(phi, hat_phi)\n",
    "\n",
    "def zernike_loss(y_true, y_pred):\n",
    "    return ALPHA * loss_object_zernike(y_true, y_pred)\n",
    "\n",
    "def zernike2gradient(coef):\n",
    "    Phi = tf.map_fn(\n",
    "        lambda x: generate_zernike(x, normalize=False),\n",
    "        coef\n",
    "    )\n",
    "    Phi = tf.convert_to_tensor(\n",
    "        np.expand_dims(\n",
    "            np.nan_to_num(Phi), \n",
    "            axis=3\n",
    "        )\n",
    "    )\n",
    "    dx, dy = tf.image.image_gradients(Phi)\n",
    "    return dx, dy\n",
    "\n",
    "def grad_loss(y_true, y_pred):\n",
    "    dx_true, dy_true = zernike2gradient(y_true)\n",
    "    dx_pred, dy_pred = zernike2gradient(y_pred)\n",
    "    return LAMBDA * (0.5*loss_object(dx_true, dx_pred) + 0.5*loss_object(dy_true, dy_pred))\n",
    "\n",
    "def total_zernike_loss(y_true, y_pred):\n",
    "    return phase_loss(y_true, y_pred) + grad_loss(y_true, y_pred) + zernike_loss(y_true, y_pred)\n",
    "\n",
    "def model_loader(path, optimizer, loss):\n",
    "    model = tf.keras.models.load_model(path, compile=False)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss)\n",
    "    return model\n",
    "\n",
    "def generate_images(models, test_input, tar):\n",
    "\n",
    "    encoder = models[0]\n",
    "    decoder = models[1]\n",
    "    zernike_decoder = models[2]\n",
    "\n",
    "    encoded_image = encoder(test_input, training=True)\n",
    "    prediction = normaliza_pos(decoder(encoded_image, training=True))\n",
    "    zernikes = zernike_decoder(encoded_image, training=True)\n",
    "    \n",
    "    got = np.nan_to_num(\n",
    "        normaliza_pos(generate_zernike(tar[0].numpy(), normalize=False)),\n",
    "        nan=0\n",
    "    )\n",
    "    \n",
    "    generated_zernike = np.nan_to_num(\n",
    "        normaliza_pos(generate_zernike(zernikes[0].numpy(), normalize=False)), \n",
    "        nan=0\n",
    "    )\n",
    "    error = np.sqrt(np.abs(got - generated_zernike))\n",
    "\n",
    "    plt.figure(figsize=(25,15))\n",
    "\n",
    "    display_list = [\n",
    "        normaliza_pos(test_input[0]), \n",
    "        normaliza_pos(prediction[0]), \n",
    "        got, \n",
    "        generated_zernike, \n",
    "        error\n",
    "    ]\n",
    "\n",
    "    title = [\n",
    "        r'a) $I_{exp}$', \n",
    "        r'b) $\\hat I$', \n",
    "        r'c) $\\phi$',\n",
    "        r'd) $\\hat \\phi$',\n",
    "        r'e) RMSE'\n",
    "    ]\n",
    "    \n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.title(title[i], fontdict={'fontsize': 20}, y=-0.15)\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i])\n",
    "        plt.colorbar(fraction=0.046)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model_loader('../models/ae_encoder.h5',\n",
    "                       autoencoder_optimizer,\n",
    "                       ae_loss)\n",
    "decoder = model_loader('../models/ae_decoder.h5',\n",
    "                       autoencoder_optimizer,\n",
    "                       ae_loss)\n",
    "zernike_decoder = model_loader('../models/zae_zernike_decoder.h5',\n",
    "                              zernike_decoder_optimizer,\n",
    "                              total_zernike_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine Tunning Experimental Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_input, example_target in test_dataset.take(1):\n",
    "    generate_images([encoder, decoder, zernike_decoder], example_input, example_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training Step and Fit Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step_zae(input_image, target):\n",
    "    with tf.GradientTape() as zernike_decoder_tape:\n",
    "        encoded_image = encoder(input_image)\n",
    "        zernikes = zernike_decoder(encoded_image, training=True)\n",
    "\n",
    "        # Loss calculation\n",
    "        zernike_decoder_loss = zernike_loss(target, zernikes)\n",
    "        estimated_phase_loss = phase_loss(target, zernikes)\n",
    "        estimated_grad_loss = grad_loss(target, zernikes)\n",
    "\n",
    "        # Total loss\n",
    "        total_zernike_decoder_loss = zernike_decoder_loss +\\\n",
    "                                     estimated_phase_loss + \\\n",
    "                                     estimated_grad_loss\n",
    "\n",
    "    zernike_decoder_gradients = zernike_decoder_tape.gradient(\n",
    "        total_zernike_decoder_loss, \n",
    "        zernike_decoder.trainable_variables\n",
    "    )\n",
    "    \n",
    "    zernike_decoder_optimizer.apply_gradients(zip(\n",
    "        zernike_decoder_gradients, \n",
    "        zernike_decoder.trainable_variables\n",
    "    )) \n",
    "\n",
    "    return [zernike_decoder_loss, \n",
    "            estimated_phase_loss, \n",
    "            estimated_grad_loss]\n",
    "\n",
    "@tf.function()\n",
    "def validation_step_zae(input_image, target):\n",
    "    encoded_image = encoder(input_image)\n",
    "    zernikes = zernike_decoder(encoded_image, training=False)\n",
    "     # Loss calculation\n",
    "    zernike_decoder_loss = zernike_loss(target, zernikes)\n",
    "    estimated_phase_loss = phase_loss(target, zernikes)\n",
    "    estimated_grad_loss = grad_loss(target, zernikes)\n",
    "\n",
    "    # Total loss\n",
    "    total_zernike_decoder_loss = zernike_decoder_loss +\\\n",
    "                                    estimated_phase_loss + \\\n",
    "                                    estimated_grad_loss\n",
    "    return total_zernike_decoder_loss\n",
    "\n",
    "def plot_graphs_zae(record, epoch):\n",
    "    x = range(epoch)\n",
    "    fig, ax1 = plt.subplots(figsize=(10,4))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('Training loss', color=color)\n",
    "    ax1.plot(x, record['total_zernike_decoder_loss_train'], color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation loss', color=color)\n",
    "    ax2.plot(x, record['total_zernike_decoder_loss_val'], color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fit_zae(train_ds, val_ds, test_ds, epochs):\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "    record = {\n",
    "        'zernike_decoder_loss': [],\n",
    "        'estimated_phase_loss': [],\n",
    "        'estimated_grad_loss': [],\n",
    "        'total_zernike_decoder_loss_train': [], \n",
    "        'total_zernike_decoder_loss_val': [], \n",
    "        'time': []\n",
    "    }\n",
    "\n",
    "    example_input, example_target = next(iter(test_ds.take(1)))\n",
    "    \n",
    "    lowest_loss = 1000\n",
    "    saved_epoch = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"Epoch: \", epoch + 1)\n",
    "\n",
    "        n = 0\n",
    "        for input_image, target in train_ds:\n",
    "            zernike_decoder_loss = train_step_zae(input_image,\n",
    "                                                  target)\n",
    "            if n % 10 == 0:\n",
    "                print('¬∑¬∑', end='')\n",
    "            n += 1\n",
    "\n",
    "        accumulated_loss_val = 0\n",
    "        for input_image, target in val_ds:\n",
    "            zernike_decoder_loss_val = validation_step_zae(input_image, \n",
    "                                                           target)\n",
    "            accumulated_loss_val += zernike_decoder_loss_val.numpy()\n",
    "        accumulated_loss_val /= len(val_ds)\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        generate_images([encoder, decoder, zernike_decoder], \n",
    "                        example_input, \n",
    "                        example_target)\n",
    "        \n",
    "        record['zernike_decoder_loss'].append(zernike_decoder_loss[0].numpy())\n",
    "        record['estimated_phase_loss'].append(zernike_decoder_loss[1].numpy())\n",
    "        record['estimated_grad_loss'].append(zernike_decoder_loss[2].numpy())\n",
    "        record['total_zernike_decoder_loss_train'].append(\n",
    "            zernike_decoder_loss[0].numpy() + \\\n",
    "            zernike_decoder_loss[1].numpy() + \\\n",
    "            zernike_decoder_loss[2].numpy()\n",
    "        )\n",
    "        record['total_zernike_decoder_loss_val'].append(accumulated_loss_val)\n",
    "        delta_time = time.time() - start\n",
    "        record['time'].append(delta_time)\n",
    "\n",
    "        plot_graphs_zae(record, epoch + 1)\n",
    "\n",
    "        print(\"Time taken: \", delta_time)\n",
    "        print(\"Zernike Decoder Loss: \", zernike_decoder_loss[0].numpy())\n",
    "        print(\"Estimated Phase Loss: \", zernike_decoder_loss[1].numpy())\n",
    "        print(\"Estimated Gradient Loss: \", zernike_decoder_loss[2].numpy())\n",
    "        print(\"Total Zernike Decoder Loss Training: \", \n",
    "              zernike_decoder_loss[0].numpy() + \\\n",
    "              zernike_decoder_loss[1].numpy() + \\\n",
    "              zernike_decoder_loss[2].numpy())\n",
    "        print(\"Total Zernike Decoder Loss Validation: \", accumulated_loss_val)\n",
    "        \n",
    "        # if epoch > round(epochs * 0.8):\n",
    "        if record['total_zernike_decoder_loss_train'][-1] < lowest_loss:\n",
    "            zernike_decoder.save('../saved_models/zae_zernike_decoder_experimental_temp.h5')\n",
    "            saved_epoch = epoch + 1\n",
    "            lowest_loss = record['total_zernike_decoder_loss_train'][-1]\n",
    "        \n",
    "        print(\"Saved model on epoch: \", saved_epoch)\n",
    "        print(\"Total Zernike Decoder Loss: \", lowest_loss)\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_config.enable_numpy_behavior()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "EPOCHS = 1\n",
    "record = fit_zae(train_ds, val_dataset, test_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 4.1 Save model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "with open('../data/record.pkl', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "\n",
    "# zernike_decoder.save('../models/zae_zernike_decoder_experimental.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation of Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zautoencoder = tf.keras.models.Model(\n",
    "    inputs=encoder.inputs, \n",
    "    outputs=zernike_decoder(encoder.outputs)\n",
    ")\n",
    "zautoencoder.compile(optimizer=zernike_decoder_optimizer, \n",
    "                     loss=total_zernike_loss)\n",
    "zautoencoder.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_estimator(y_true, y_pred):\n",
    "    return np.sqrt(np.abs(y_true - y_pred))\n",
    "\n",
    "samples = [1, 2, 3, 4, 5]\n",
    "example_input, example_target = next(iter(train_ds.take(100)))\n",
    "zernike_coefs = zautoencoder.predict(example_input)\n",
    "generated_zernike = zernike2phi(zernike_coefs)\n",
    "real_zernike = zernike2phi(example_target)\n",
    "plt.figure(figsize=(20, 16))\n",
    "for i in range(5):\n",
    "    y_true = normaliza_pos(real_zernike[samples[i]])*MASK\n",
    "    y_pred = normaliza_pos(generated_zernike[samples[i]])*MASK\n",
    "    error = error_estimator(y_true, y_pred)*MASK\n",
    "    experimental_pattern = example_input[samples[i]]\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(experimental_pattern, origin='lower')\n",
    "    # plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$I_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "    plt.subplot(4, 5, i + 6)\n",
    "    plt.imshow(y_true, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\phi_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(y_pred, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\hat\\phi_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.18)\n",
    "    plt.subplot(4, 5, i + 16)\n",
    "    plt.imshow(error, origin='lower')\n",
    "    plt.colorbar(fraction=0.046)\n",
    "    plt.axis('off')\n",
    "    plt.title(r'$\\epsilon_{}$'.format(i + 1), fontdict={'fontsize': 20}, y=-0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
